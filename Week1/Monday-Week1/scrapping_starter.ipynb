{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scrapping using python\n",
    "\n",
    "#### References\n",
    "1. [Practical Introduction to Web Scraping in Python](https://realpython.com/python-web-scraping-practical-introduction/)\n",
    "2. [Web Scraping using Python](https://www.datacamp.com/community/tutorials/web-scraping-using-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $ python3 -m venv venv\n",
    "# $ . ./venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fire\n",
      "  Using cached https://files.pythonhosted.org/packages/34/a7/0e22e70778aca01a52b9c899d9c145c6396d7b613719cd63db97ffa13f2f/fire-0.3.1.tar.gz\n",
      "Requirement already satisfied: six in /Users/yabebal/.conda/envs/10x/lib/python3.7/site-packages (from fire) (1.12.0)\n",
      "Collecting termcolor (from fire)\n",
      "  Using cached https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Building wheels for collected packages: fire, termcolor\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/yabebal/Library/Caches/pip/wheels/c1/61/df/768b03527bf006b546dce284eb4249b185669e65afc5fbb2ac\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/yabebal/Library/Caches/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "Successfully built fire termcolor\n",
      "Installing collected packages: termcolor, fire\n",
      "Successfully installed fire-0.3.1 termcolor-1.1.0\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 20.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Better\n",
    "!pip install requests BeautifulSoup4 fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from requests.exceptions import RequestException\n",
    "from contextlib import closing\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "\n",
    "import fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile ../pyscrap_url.py\n",
    "\n",
    "def simple_get(url):\n",
    "    \"\"\"\n",
    "    Attempts to get the content at `url` by making an HTTP GET request.\n",
    "    If the content-type of response is some kind of HTML/XML, return the\n",
    "    text content, otherwise return None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with closing(get(url, stream=True)) as resp:\n",
    "            if is_good_response(resp):\n",
    "                return resp.content  #.encode(BeautifulSoup.original_encoding)\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "    except RequestException as e:\n",
    "        log_error('Error during requests to {0} : {1}'.format(url, str(e)))\n",
    "        return None\n",
    "\n",
    "\n",
    "def is_good_response(resp):\n",
    "    \"\"\"\n",
    "    Returns True if the response seems to be HTML, False otherwise.\n",
    "    \"\"\"\n",
    "    content_type = resp.headers['Content-Type'].lower()\n",
    "    return (resp.status_code == 200 \n",
    "            and content_type is not None \n",
    "            and content_type.find('html') > -1)\n",
    "\n",
    "\n",
    "def log_error(e):\n",
    "    \"\"\"\n",
    "    It is always a good idea to log errors. \n",
    "    This function just prints them, but you can\n",
    "    make it do anything.\n",
    "    \"\"\"\n",
    "    print(e)\n",
    "    \n",
    "def get_elements(url, tag='',search={}, fname=None):\n",
    "    \"\"\"\n",
    "    Downloads a page specified by the url parameter\n",
    "    and returns a list of strings, one per tag element\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(url,str):\n",
    "        response = simple_get(url)\n",
    "    else:\n",
    "        #if already it is a loaded html page\n",
    "        response = url\n",
    "\n",
    "    if response is not None:\n",
    "        html = BeautifulSoup(response, 'html.parser')\n",
    "        \n",
    "        res = []\n",
    "        if tag:    \n",
    "            for li in html.select(tag):\n",
    "                for name in li.text.split('\\n'):\n",
    "                    if len(name) > 0:\n",
    "                        res.append(name.strip())\n",
    "                       \n",
    "                \n",
    "        if search:\n",
    "            soup = html            \n",
    "            \n",
    "            \n",
    "            r = ''\n",
    "            if 'find' in search.keys():\n",
    "                print('findaing',search['find'])\n",
    "                soup = soup.find(**search['find'])\n",
    "                r = soup\n",
    "\n",
    "                \n",
    "            if 'find_all' in search.keys():\n",
    "                print('findaing all of',search['find_all'])\n",
    "                r = soup.find_all(**search['find_all'])\n",
    "   \n",
    "            if r:\n",
    "                for x in list(r):\n",
    "                    if len(x) > 0:\n",
    "                        res.extend(x)\n",
    "            \n",
    "        return res\n",
    "\n",
    "    # Raise an exception if we failed to get any data from the url\n",
    "    raise Exception('Error retrieving contents at {}'.format(url))    \n",
    "    \n",
    "    \n",
    "if get_ipython().__class__.__name__ == '__main__':\n",
    "    fire(get_tag_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = get_elements('https://africafreak.com/100-most-influential-twitter-users-in-africa', tag='h2')\n",
    "top100 = res[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "url= 'https://www.atlanticcouncil.org/blogs/africasource/african-leaders-respond-to-coronavirus-on-twitter/#east-africa'\n",
    "response = simple_get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "findaing all of {'class_': 'twitter-tweet'}\n"
     ]
    }
   ],
   "source": [
    "# wp-block-embed__wrapper\n",
    "res = get_elements(response, search={'find_all':{'class_':'twitter-tweet'}})\n",
    "tag_strings = []\n",
    "for tag in res:\n",
    "    if tag.string != None:\n",
    "        tag_strings.append(tag.string)\n",
    "tag_strings.extend(top100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "handleregex = re.compile(r'@[a-zA-Z0-9_]{0,15}')\n",
    "tags = ''.join(tag_strings)\n",
    "groups = handleregex.findall(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handles = []\n",
    "for h in groups:\n",
    "    h = h.replace('@','')\n",
    "    handles.append(h)\n",
    "\n",
    "len(handles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = os.environ.get('TWITTER_API_KEY')\n",
    "consumer_secret = os.environ.get('TWITTER_API_SECRET')\n",
    "access_token = os.environ.get('TWITTER_ACCESS_TOKEN')\n",
    "access_token_secret = os.environ.get('TWITTER_ACCESS_TOKEN_SECRET')\n",
    "\n",
    "# This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131\n",
      "125\n"
     ]
    }
   ],
   "source": [
    "new=[]\n",
    "for h in handles:\n",
    "    try:\n",
    "        user = api.get_user(h)\n",
    "        new.append(h)\n",
    "    except:\n",
    "        handles.remove(h)\n",
    "print(len(handles))\n",
    "print(len(new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./scrap_tweetusers.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile ./scrap_tweetusers.ipynb\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import preprocessor as p\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "class tweetsearch():\n",
    "    '''\n",
    "    This is a basic class to search and download twitter data.\n",
    "    You can build up on it to extend the functionalities for more \n",
    "    sophisticated analysis\n",
    "    '''\n",
    "    def __init__(self, cols=None,auth=None):\n",
    "        #\n",
    "        if not cols is None:\n",
    "            self.cols = cols\n",
    "        else:\n",
    "            self.cols = ['id', 'created_at', 'source', 'original_text','clean_text', \n",
    "                    'sentiment','polarity','subjectivity', 'lang',\n",
    "                    'favorite_count', 'retweet_count', 'original_author',   \n",
    "                    'possibly_sensitive', 'hashtags',\n",
    "                    'user_mentions', 'place', 'place_coord_boundaries']\n",
    "            \n",
    "        if auth is None:\n",
    "            # Variables that contains the user credentials to access Twitter API \n",
    "            consumer_key = os.environ.get('TWITTER_API_KEY')\n",
    "            consumer_secret = os.environ.get('TWITTER_API_SECRET')\n",
    "            access_token = os.environ.get('TWITTER_ACCESS_TOKEN')\n",
    "            access_token_secret = os.environ.get('TWITTER_ACCESS_TOKEN_SECRET')\n",
    "\n",
    "            # This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "            auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "            auth.set_access_token(access_token, access_token_secret)\n",
    "                       \n",
    "        self.auth = auth\n",
    "        self.api = tweepy.API(auth, wait_on_rate_limit=True) \n",
    "        self.filtered_tweet = ''\n",
    "            \n",
    "\n",
    "    def clean_tweets(self, twitter_text):\n",
    "\n",
    "        # use pre processor\n",
    "        tweet = p.clean(twitter_text)\n",
    "\n",
    "         # HappyEmoticons\n",
    "        emoticons_happy = set([\n",
    "            ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "            ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "            '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "            'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "            '<3'\n",
    "            ])\n",
    "\n",
    "        # Sad Emoticons\n",
    "        emoticons_sad = set([\n",
    "            ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "            ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "            ':c', ':{', '>:\\\\', ';('\n",
    "            ])\n",
    "\n",
    "        # Emoji patterns\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                 u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                 u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                 u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                 u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                 u\"\\U00002702-\\U000027B0\"\n",
    "                 u\"\\U000024C2-\\U0001F251\"\n",
    "                 \"]+\", flags=re.UNICODE)\n",
    "\n",
    "        # combine sad and happy emoticons\n",
    "        emoticons = emoticons_happy.union(emoticons_sad)\n",
    "\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        word_tokens = nltk.word_tokenize(tweet)\n",
    "        # after tweepy preprocessing the colon symbol left remain after      \n",
    "        # removing mentions\n",
    "        tweet = re.sub(r':', '', tweet)\n",
    "        tweet = re.sub(r'‚Ä¶', '', tweet)\n",
    "\n",
    "        # replace consecutive non-ASCII characters with a space\n",
    "        tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "\n",
    "        # remove emojis from tweet\n",
    "        tweet = emoji_pattern.sub(r'', tweet)\n",
    "\n",
    "        # filter using NLTK library append it to a string\n",
    "        filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "        # looping through conditions\n",
    "        filtered_tweet = []    \n",
    "        for w in word_tokens:\n",
    "        # check tokens against stop words , emoticons and punctuations\n",
    "            if w not in stop_words and w not in emoticons and w not in string.punctuation:\n",
    "                filtered_tweet.append(w)\n",
    "\n",
    "        return ' '.join(filtered_tweet)            \n",
    "\n",
    "    def get_tweets(self, handles, csvfile=None):\n",
    "        \n",
    "        df = pd.DataFrame(columns=self.cols)\n",
    "        \n",
    "        if not csvfile is None:\n",
    "            # If the file exists, then read the existing data from the CSV file.\n",
    "            if os.path.exists(csvfile):\n",
    "                df = pd.read_csv(csvfile, header=0)\n",
    "            \n",
    "        # page attribute in tweepy.cursor and iteration\n",
    "        for handle in handles:\n",
    "            # the you receive from the Twitter API is in a JSON format and has quite an amount of information attached\n",
    "            for status in self.api.user_timeline(id=handle,count=100, include_rts=False): \n",
    "                new_entry = []\n",
    "                status = status._json               \n",
    "                # filter by language\n",
    "                # if status['lang'] != 'en':\n",
    "                \n",
    "                #    continue\n",
    "\n",
    "                # if this tweet is a retweet update retweet count\n",
    "                if status['created_at'] in df['created_at'].values:\n",
    "                    i = df.loc[df['created_at'] == status['created_at']].index[0]\n",
    "                    #\n",
    "                    cond1 = status['favorite_count'] != df.at[i, 'favorite_count']\n",
    "                    cond2 = status['retweet_count'] != df.at[i, 'retweet_count']\n",
    "                    if cond1 or cond2:\n",
    "                        df.at[i, 'favorite_count'] = status['favorite_count']\n",
    "                        df.at[i, 'retweet_count'] = status['retweet_count']\n",
    "                    continue\n",
    "\n",
    "                #calculate sentiment\n",
    "                filtered_tweet = self.clean_tweets(status['text'])\n",
    "                blob = TextBlob(filtered_tweet)\n",
    "                Sentiment = blob.sentiment     \n",
    "                polarity = Sentiment.polarity\n",
    "                subjectivity = Sentiment.subjectivity\n",
    "\n",
    "                new_entry += [status['id'], status['created_at'],\n",
    "                              status['source'], status['text'], filtered_tweet, \n",
    "                              Sentiment, polarity, subjectivity, status['lang'],\n",
    "                              status['favorite_count'], status['retweet_count']]\n",
    "\n",
    "                new_entry.append(status['user']['screen_name'])\n",
    "\n",
    "                try:\n",
    "                    is_sensitive = status['possibly_sensitive']\n",
    "                except KeyError:\n",
    "                    is_sensitive = None\n",
    "\n",
    "                new_entry.append(is_sensitive)\n",
    "\n",
    "                hashtags = \", \".join([hashtag_item['text'] for hashtag_item in status['entities']['hashtags']])\n",
    "                new_entry.append(hashtags)  # append the hashtags\n",
    "\n",
    "                #\n",
    "                mentions = \", \".join([mention['screen_name'] for mention in status['entities']['user_mentions']])\n",
    "                new_entry.append(mentions)  # append the user mentions\n",
    "\n",
    "                try:\n",
    "                    xyz = status['place']['bounding_box']['coordinates']\n",
    "                    coordinates = [coord for loc in xyz for coord in loc]\n",
    "                except TypeError:\n",
    "                    coordinates = None\n",
    "                #\n",
    "                new_entry.append(coordinates)\n",
    "\n",
    "                try:\n",
    "                    location = status['user']['location']\n",
    "                except TypeError:\n",
    "                    location = ''\n",
    "                #\n",
    "                new_entry.append(location)\n",
    "\n",
    "                #now append a row to the dataframe\n",
    "                single_tweet_df = pd.DataFrame([new_entry], columns=self.cols)\n",
    "                df = df.append(single_tweet_df, ignore_index=True)\n",
    "\n",
    "        df['timestamp'] = df.created_at.map(pd.Timestamp)\n",
    "        df = df.sort_values('timestamp').set_index('timestamp')\n",
    "        df = df.drop('id',axis=1)\n",
    "        \n",
    "        if not csvfile is None:\n",
    "            #save it to file\n",
    "            df.to_csv(csvfile, index=True, encoding=\"utf-8\")\n",
    "            \n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 8204 entries, 2009-07-03 22:33:37+00:00 to 2020-07-15 10:21:30+00:00\n",
      "Data columns (total 16 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   created_at              8204 non-null   object \n",
      " 1   source                  8204 non-null   object \n",
      " 2   original_text           8204 non-null   object \n",
      " 3   clean_text              8204 non-null   object \n",
      " 4   sentiment               8204 non-null   object \n",
      " 5   polarity                8204 non-null   float64\n",
      " 6   subjectivity            8204 non-null   float64\n",
      " 7   lang                    8204 non-null   object \n",
      " 8   favorite_count          8204 non-null   object \n",
      " 9   retweet_count           8204 non-null   object \n",
      " 10  original_author         8204 non-null   object \n",
      " 11  possibly_sensitive      5863 non-null   object \n",
      " 12  hashtags                8204 non-null   object \n",
      " 13  user_mentions           8204 non-null   object \n",
      " 14  place                   272 non-null    object \n",
      " 15  place_coord_boundaries  8204 non-null   object \n",
      "dtypes: float64(2), object(14)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "tweets_file = 'data/influencersandleaders.json'\n",
    "\n",
    "\n",
    "##get data on keywords\n",
    "if os.path.exists(tweets_file):\n",
    "    #get file if you have already downloaded what you wanted\n",
    "    df = pd.read_csv(tweets_file, header=0)\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = df.created_at.map(pd.Timestamp)\n",
    "        df = df.sort_values('timestamp').set_index('timestamp')\n",
    "        df = df.drop('id',axis=1)    \n",
    "else:\n",
    "    ts = tweetsearch()\n",
    "    df = ts.get_tweets(new, csvfile=tweets_file)    #you saved the \n",
    "\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scrapping using bash script\n",
    "If the web site has a quite simple HTML, you can easily use curl to perform the request and then extract the needed values using bash commands grep, cut , sed, ..\n",
    "\n",
    "This tutorial is adapted from [this](https://medium.com/@LiliSousa/web-scraping-with-bash-690e4ee7f98d) medium article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<blockquote class=\"twitter-tweet\" data-width=\"550\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">The Deputy Prime Minister Themba Masuku has today met representatives of the private sector and employees&#39 <blockquote class=\"twitter-tweet\" data-width=\"550\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">GUIDELINES FOR SCHOOLS IN <a href=\"https://twitter.com/hashtag/MALAWI?src=hash&amp <blockquote class=\"twitter-tweet\" data-width=\"550\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">Fellow Namibians, I declared a State of Emergency on <a href=\"https://twitter.com/hashtag/COVID19?src=hash&amp <blockquote class=\"twitter-tweet\" data-width=\"550\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\"><a href=\"https://twitter.com/hashtag/COVID19measuresSC?src=hash&amp <blockquote class=\"twitter-tweet\" data-width=\"550\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">The Minister for Cooperative Governance &amp <blockquote class=\"twitter-tweet\" data-width=\"550\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">Join the <a href=\"https://twitter.com/hashtag/SafeHands?src=hash&amp <blockquote class=\"twitter-tweet\" data-width=\"550\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">I urge my fellow Zimbabweans to maintain excellent levels of personal hygiene. Wash your hands thoroughly with soap, cover your nose &amp <blockquote class=\"twitter-tweet\" data-width=\"550\" data-dnt=\"true\"><p lang=\"fr\" dir=\"ltr\"><a href=\"https://twitter.com/hashtag/COVID19DJ?src=hash&amp <blockquote class=\"twitter-tweet\" data-width=\"550\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">The Ministry of Health announced this evening the first confirmed case of a Coronavirus patient who arrived at Asmara International Airport from Norway with Fly Dubai at 7:00 a.m. LT this morning. The 39-year old patient is an Eritrean national with permanent residence in Norway</p>&mdash <blockquote class=\"twitter-tweet\" data-width=\"550\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">Please join His Excellency President Uhuru Kenyatta this Saturday, 21st March 2020 at 12 noon for a broadcast prayer service to mark the National Day of Prayer on the Coronavirus pandemic. The Service will be led by a cross-section of religious leaders. <a href=\"https://t.co/kDZPDUpeGz\">pic.twitter.com/kDZPDUpeGz</a></p>&mdash <blockquote class=\"twitter-tweet\" data-width=\"550\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">I joined <a href=\"https://twitter.com/WHO?ref_src=twsrc%5Etfw\">@WHO</a> <a href=\"https://twitter.com/hashtag/SafeHands?src=hash&amp <blockquote class=\"twitter-tweet\" data-width=\"550\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">We commend all medical practitioners across the globe working hard to counter the <a href=\"https://twitter.com/hashtag/CoronavirusPandemic?src=hash&amp <blockquote class=\"twitter-tweet\" data-width=\"550\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">20 March 2020: Communication from the <a href=\"https://twitter.com/hashtag/SouthSudan?src=hash&amp <blockquote class=\"twitter-tweet\" data-width=\"550\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">There is no shame in not shaking hands and in social distancing, during these difficult times, we must protect ourselves and those around us.<br>Prevention is the best cure, therefore please be sure to follow health safety and security instructions.<a href=\"https://twitter.com/hashtag/%D8%A7%D8%AD%D8%AA%D9%8A%D8%A7%D8%B7%D8%A7%D8%AA_%D8%A7%D9%84%D8%B3%D9%88%D8%AF%D8%A7%D9%86_%D9%84%D9%85%D9%86%D8%B9_%D9%83%D9%88%D8%B1%D9%88%D9%86%D8%A7?src=hash&amp <blockquote class=\"twitter-tweet\" data-width=\"550\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">We have confirmed first case of Corona patient in Tanzania. The government was prepared with isolation centers and isolation hospitals. More measures to curb the spread will continue to be announced. In the meantime, let us continue to observe health precautions.<br><br>Dr. H.A</p>&mdash <blockquote class=\"twitter-tweet\" data-width=\"550\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">We believe that we can never do enough in the face of corona virus unless God is with us.<br><br>We shall hold prayers as a nation at State House, Entebbe.<br> <br>Join us from wherever you will be <a href=\"https://t.co/voLCrF8nPK\">pic.twitter.com/voLCrF8nPK</a></p>&mdash <blockquote class=\"twitter-tweet\" data-width=\"550\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">Minister Manuel Augusto participating at SADC Council of Ministers Meeting from the Ministry in Luanda through videoconference, due to the Coronavirus outbreak. The meeting was conducted from Dar Es Salaam,Tanzania from 16 to 18/3. Tanzania holds the current presidency of SADC. <a href=\"https://t.co/vOfLwRizKr\">pic.twitter.com/vOfLwRizKr</a></p>&mdash <blockquote class=\"twitter-tweet\" data-width=\"550\" data-dnt=\"true\"><p lang=\"fr\" dir=\"ltr\">🔴 Une rumeur de mauvais goût sur le <a href=\"https://twitter.com/hashtag/coronavirus?src=hash&amp <blockquote class=\"twitter-tweet\" data-width=\"550\" data-dnt=\"true\"><p lang=\"fr\" dir=\"ltr\">La situation du premier patient testé positif au COVID-19 et pris en charge depuis hier par les services médicaux compétents, s&#39 <blockquote class=\"twitter-tweet\" data-width=\"550\" data-dnt=\"true\"><p lang=\"und\" dir=\"ltr\"><a href=\"https://twitter.com/hashtag/RDC?src=hash&amp <blockquote class=\"twitter-tweet\" data-width=\"550\" data-dnt=\"true\"><p lang=\"fr\" dir=\"ltr\">Pour ralentir la diffusion du Covid-19 au Gabon et ainsi en atténuer au maximum l&#39 <blockquote class=\"twitter-tweet\" data-width=\"550\" data-dnt=\"true\"><p lang=\"fr\" dir=\"ltr\">Pour réduire les risques de propagation du <a href=\"https://twitter.com/hashtag/Coronavirus?src=hash&amp <blockquote class=\"twitter-tweet\" data-width=\"550\" data-dnt=\"true\"><p lang=\"fr\" dir=\"ltr\">J&#39 <blockquote class=\"twitter-tweet\" data-width=\"550\" data-dnt=\"true\"><p lang=\"und\" dir=\"ltr\"><a href=\"https://twitter.com/hashtag/preven%C3%A7%C3%A3ocovid19?src=hash&amp <blockquote class=\"twitter-tweet\" data-width=\"550\" data-dnt=\"true\"><p lang=\"fr\" dir=\"ltr\">Mes chers compatriotes, face à la pandémie du Coronavirus COVID-19, adoptons les bonnes pratiques et mettons en application les mesures de prévention recommandées par le Ministère de la Santé et l’OMS. <a href=\"https://t.co/cSxzODR6Vs\">https://t.co/cSxzODR6Vs</a> <a href=\"https://t.co/tILcYxVCf8\">pic.twitter.com/tILcYxVCf8</a></p>&mdash <blockquote class=\"twitter-tweet\" data-width=\"550\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\"><a href=\"https://twitter.com/hashtag/COVID19?src=hash&amp <blockquote class=\"twitter-tweet\" data-width=\"550\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">Here are excerpts of my address to the nation on the updates taken to combat the spread of Coronavirus <a href=\"https://t.co/dkfcrrEiy5\">pic.twitter.com/dkfcrrEiy5</a></p>&mdash <blockquote class=\"twitter-tweet\" data-width=\"550\" data-dnt=\"true\"><p lang=\"fr\" dir=\"ltr\">Dans un contexte de crise <a href=\"https://twitter.com/hashtag/sanitaire?src=hash&amp <blockquote class=\"twitter-tweet\" data-width=\"550\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\"><a href=\"https://twitter.com/hashtag/GuineaBissau?src=hash&amp <blockquote class=\"twitter-tweet\" data-width=\"550\" data-dnt=\"true\"><p lang=\"fr\" dir=\"ltr\">Communiqué intégrale du Conseil Extraordinaire de Défense présidé ce 17 Mars par le Chef de l’Etat sur le <a href=\"https://twitter.com/hashtag/coronavirus?src=hash&amp <blockquote class=\"twitter-tweet\" data-width=\"550\" data-dnt=\"true\"><p lang=\"fr\" dir=\"ltr\">Chers citoyens et résidents, <br>Un cas de Covid-19 concernant un citoyen étranger ayant été confirmé, je voudrais vous rassurer que les dispositions ont été prises à tous les niveaux. Je vous engage à la prudence et au strict respect des directives des autorités compétentes.</p>&mdash <blockquote class=\"twitter-tweet\" data-width=\"550\" data-dnt=\"true\"><p lang=\"fr\" dir=\"ltr\">Mes Chers Concitoyens, <br>Depuis la grippe Espagnole, il y a un siècle, l’ humanité n’a pas connu un fléau sanitaire d’ une telle ampleur. Je le répète: il n’y a ni traitement, ni vaccin. Notre seule arme reste la prévention. <a href=\"https://t.co/akIhqXETrW\">https://t.co/akIhqXETrW</a></p>&mdash <blockquote class=\"twitter-tweet\" data-width=\"550\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">Protecting Nigerians from the Coronavirus is a key priority for us as a Government. We have the Ministry of Health and the Nigeria Center for Disease Control (NCDC) working round-the-clock with several other agencies, as well as State Governments, to ensure this.</p>&mdash <blockquote class=\"twitter-tweet\" data-width=\"550\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">Dear President <a href=\"https://twitter.com/PaulKagame?ref_src=twsrc%5Etfw\">@PaulKagame</a>, I accept the challenge and encourage all people from Senegal and all contaminated countries to do as well against <a href=\"https://twitter.com/hashtag/COVID19?src=hash&amp <blockquote class=\"twitter-tweet\" data-width=\"550\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">Although there are no confirmed <a href=\"https://twitter.com/hashtag/Coronavirus?src=hash&amp <blockquote class=\"twitter-tweet\" data-width=\"550\" data-dnt=\"true\"><p lang=\"ca\" dir=\"ltr\">Manifestations coronavirus <a href=\"https://t.co/mdjyRWUuoq\">pic.twitter.com/mdjyRWUuoq</a></p>&mdash\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "# curl the page and save content to tmp_file\n",
    "#url = \"https://www.atlanticcouncil.org/blogs/africasource/african-leaders-respond-to-coronavirus-on-twitter/#east-africa\"\n",
    "#curl -X GET $url -o tmp_file\n",
    "\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "# write headers to CSV file\n",
    "echo \"Name, twitter_id\" >> extractData.csv\n",
    "n=\"1\"\n",
    "while [ $n -lt 2 ]\n",
    "do\n",
    "  \n",
    "  #get title\n",
    "  title=$(cat tmp_file | grep \"class=\\\"twitter-tweet\\\"\" | cut -d ';' -f1 )\n",
    "  echo $title\n",
    "  #get author\n",
    "  #twitter_id=$(cat tmp_file |grep -A1 \"class=\\\"css-901oao css-16my406 r-1qd0xha r-ad9z0x r-bcqeeo r-qvutc0\\\"\" | tail -1)\n",
    "\n",
    "  #echo \"$title, $twitter_id\" >> extractData.csv\n",
    "  #echo \"$title, $twitter_id\"\n",
    "    \n",
    "  n=$[$n+1]\n",
    "\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
